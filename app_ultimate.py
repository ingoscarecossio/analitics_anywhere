# -*- coding: utf-8 -*-
import os, json, time
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime

from ANALYTICS_ULT.io_utils import load_table, coerce_booleans, coerce_datetimes, coerce_numeric
from ANALYTICS_ULT.path_utils import split_path_to_levels, path_depth_from_levels
from ANALYTICS_ULT.analyzers import (overview_metrics, top_n_by_size, missingness, freq_table, duplicates_by_hash, timeline_counts, agg_by_folder, agg_by)
from ANALYTICS_ULT.mismatch import mime_ext_mismatch
from ANALYTICS_ULT.validators import validate_sizes, validate_dates, anomalies_size_iqr
from ANALYTICS_ULT.security import octal_to_rwx
from ANALYTICS_ULT.risk import risk_scoring, DEFAULT_POLICIES
from ANALYTICS_ULT.simulator import simulate_dedupe
from ANALYTICS_ULT.viz import bar_chart, line_chart, hist_log_sizes, treemap_sliced, heatmap_pivot, smart_time_series, bar_top
from ANALYTICS_ULT.categorize import add_category_column
from ANALYTICS_ULT.analyzers import size_buckets, kpi_advanced
from ANALYTICS_ULT.exporters import export_excel_with_figs

st.set_page_config(page_title="Anywhere Analytics ULTIMATE", layout="wide")
st.title("üèÜ Anywhere Analytics ULTIMATE")
st.caption("UI avanzada + simulador de deduplicaci√≥n + explicabilidad de riesgos + exportes ejecutivos.")

with st.sidebar:
    st.header("üìÅ Datos de entrada")
    default_path = st.text_input("Ruta por defecto (opcional):", value="")
    uploaded = st.file_uploader("Corte actual (Excel/CSV)", type=["xlsx","xls","xlsm","xlsb","csv","txt"])
    baseline = st.file_uploader("Corte base (opcional)", type=["xlsx","xls","xlsm","xlsb","csv","txt"])
    sheet_name = st.text_input("Hoja (si Excel):", value="")
    sep = st.text_input("Separador (si CSV):", value=",")
    encoding = st.text_input("Encoding (si CSV):", value="utf-8")
    st.markdown("---")
    st.header("üß† Policies (Risk)")
    policies_json = st.text_area("JSON de policies", value=json.dumps(DEFAULT_POLICIES, indent=2), height=280)

def _load_dataframe(file, default_path, sheet_name, sep, encoding):
    if file is not None:
        tmp_dir = os.path.join(".", "__tmp__"); os.makedirs(tmp_dir, exist_ok=True)
        in_path = os.path.join(tmp_dir, file.name)
        with open(in_path, "wb") as f: f.write(file.getbuffer())
        df = load_table(in_path, sheet_name=sheet_name or None, sep=sep or ",", encoding=encoding or "utf-8")
    elif default_path and os.path.exists(default_path):
        df = load_table(default_path, sheet_name=sheet_name or None, sep=sep or ",", encoding=encoding or "utf-8")
    else:
        st.stop()
    df = coerce_datetimes(df, ["FechaCreacion","FechaModificacion","FechaAcceso"])
    df = coerce_numeric(df, ["TamanoBytes"])
    df = coerce_booleans(df, ["Oculto","SoloLectura"])
    df = add_category_column(df)
    if not any(c.startswith("Nivel_") for c in df.columns):
        if "RutaRelativa" in df.columns:
            levels = split_path_to_levels(df["RutaRelativa"])
            df = pd.concat([df.reset_index(drop=True), levels.reset_index(drop=True)], axis=1)
    df["Profundidad"] = path_depth_from_levels(df)
    if "PermOctal" in df.columns:
        df["Perm_RWX"] = df["PermOctal"].apply(octal_to_rwx)
    base_ruta = "RutaCompleta" if "RutaCompleta" in df.columns else ("RutaRelativa" if "RutaRelativa" in df.columns else None)
    df["LongRuta"] = df[base_ruta].astype(str).str.len() if base_ruta else np.nan
    return df

df = _load_dataframe(uploaded, default_path, sheet_name, sep, encoding)
df_base = _load_dataframe(baseline, default_path, sheet_name, sep, encoding) if baseline is not None else None

# Bookmarks (guardar/cargar vistas)
st.sidebar.markdown("---")
st.sidebar.header("üîñ Bookmarks")
if "bookmark" not in st.session_state: st.session_state["bookmark"] = {}
bookmark_name = st.sidebar.text_input("Nombre de bookmark", value="mi_vista")
if st.sidebar.button("Guardar bookmark"):
    st.session_state["bookmark"][bookmark_name] = {
        "columns": list(df.columns),
        "sample_head": df.head(5).to_dict(orient="records")
    }
if st.sidebar.button("Exportar bookmarks JSON"):
    st.sidebar.download_button("Descargar JSON", data=json.dumps(st.session_state["bookmark"], indent=2).encode("utf-8"), file_name="bookmarks.json")

# Tabs
tab_dash, tab_kpis, tab_risk, tab_dup, tab_folders, tab_heatmap, tab_time, tab_quality, tab_mismatch, tab_delta, tab_validate, tab_export = st.tabs([
    "Dashboard","KPIs+","Riesgos","Duplicados/Simulador","Carpetas","Heatmap","Temporal","Calidad","MIME vs Ext","Delta","Validaciones","Exportar"
])

with tab_dash:
    
    st.subheader("KPIs")
    met = overview_metrics(df)
    k = st.columns(4)
    k[0].metric("Archivos", f"{met.get('filas', len(df)):,}")
    k[1].metric("Tama√±o total", met.get("tamano_total_humano","‚Äî"))
    k[2].metric("0 bytes", f"{met.get('archivos_cero_bytes',0):,}")
    k[3].metric("Extensiones √∫nicas", f"{met.get('extensiones_unicas','‚Äî')}")
    st.markdown("**Top por tama√±o**"); st.dataframe(top_n_by_size(df, n=50), use_container_width=True, height=360)
    if "TamanoBytes" in df.columns:
        fig = hist_log_sizes(df["TamanoBytes"]); 
        if fig is not None: st.pyplot(fig, use_container_width=True)


with tab_kpis:
    st.subheader("KPIs de impacto y categor√≠as")
    # Conteo y tama√±o por categor√≠a
    if "Categoria" in df.columns:
        cat_counts = df["Categoria"].value_counts().reset_index().rename(columns={"index":"Categoria","Categoria":"conteo"})
        st.markdown("**Archivos por categor√≠a**")
        st.dataframe(cat_counts, use_container_width=True, height=260)
        try:
            st.pyplot(bar_top(cat_counts, "Categoria", "conteo", "Top categor√≠as por n√∫mero de archivos", horizontal=True), use_container_width=True)
        except Exception:
            pass
        # Tama√±o por categor√≠a
        if "TamanoBytes" in df.columns:
            tmp = df.copy(); tmp["TamanoBytes"] = pd.to_numeric(tmp["TamanoBytes"], errors="coerce")
            cat_size = tmp.groupby("Categoria")["TamanoBytes"].sum().sort_values(ascending=False).reset_index()
            st.markdown("**Tama√±o total (bytes) por categor√≠a**")
            st.dataframe(cat_size, use_container_width=True, height=260)
    # Buckets de tama√±o
    sb = size_buckets(df)
    if not sb.empty:
        st.markdown("**Distribuci√≥n por rangos de tama√±o**")
        st.dataframe(sb, use_container_width=True, height=200)
        try:
            st.pyplot(bar_top(sb, "rango", "conteo", "Archivos por rango de tama√±o"), use_container_width=True)
        except Exception:
            pass
    # KPIs avanzados
    st.markdown("**KPIs Avanzados**")
    st.dataframe(kpi_advanced(df), use_container_width=True, height=240)

with tab_risk:
    st.subheader("Priorizaci√≥n + explicaci√≥n")
    try: policies = json.loads(policies_json)
    except Exception as e:
        st.error(f"Policies JSON inv√°lido: {e}"); policies = DEFAULT_POLICIES
    scored = risk_scoring(df, policies)
    st.dataframe(scored[["Nombre","TamanoBytes","Perm_RWX","LongRuta","Profundidad","RiskScore","RiskBand","RiskWhy"]].head(1000), use_container_width=True, height=420)

with tab_dup:
    st.subheader("Duplicados por Hash/PseudoHash + Simulador")
    dup, espacio = duplicates_by_hash(df)
    st.write(f"**Espacio potencial recuperable (ingenuo):** {espacio:,.0f} bytes")
    st.dataframe(dup, use_container_width=True, height=300)
    st.markdown("---")
    st.markdown("### Simulador de deduplicaci√≥n")
    by = st.selectbox("Agrupar por", options=[c for c in ["CarpetaPadre","Propietario","Extension","Raiz"] if c in df.columns])
    strat = st.selectbox("Estrategia", options=["keep-largest","keep-earliest","keep-latest"])
    if st.button("Simular"):
        plan, ahorro = simulate_dedupe(df, by=by, strategy=strat)
        st.metric("Ahorro estimado", f"{ahorro:,.0f} bytes")
        st.dataframe(plan.head(1000), use_container_width=True, height=360)
        st.download_button("Descargar plan CSV", data=plan.to_csv(index=False).encode("utf-8"), file_name="plan_deduplicacion.csv")

with tab_folders:
    st.subheader("Agregaci√≥n por carpeta / ra√≠z / extensi√≥n")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("**Por carpeta (Top 50)**"); st.dataframe(agg_by_folder(df, top=50), use_container_width=True, height=360)
    with c2:
        if "Raiz" in df.columns:
            st.markdown("**Por ra√≠z**"); st.dataframe(agg_by(df, "Raiz", top=50), use_container_width=True, height=360)
        if "Extension" in df.columns:
            st.markdown("**Por extensi√≥n (Top 30)**"); st.dataframe(agg_by(df, "Extension", top=30), use_container_width=True, height=360)
    if "CarpetaPadre" in df.columns:
        tt = df.groupby("CarpetaPadre")["TamanoBytes"].sum().sort_values(ascending=False).head(25)
        fig = treemap_sliced(tt.values, tt.index, title="Treemap - Top carpetas por tama√±o")
        if fig is not None: st.pyplot(fig, use_container_width=True)

with tab_heatmap:
    st.subheader("Heatmap de tama√±os por Propietario vs Extensi√≥n")
    if "Propietario" in df.columns and "Extension" in df.columns and "TamanoBytes" in df.columns:
        pv = pd.pivot_table(df, values="TamanoBytes", index="Propietario", columns="Extension", aggfunc=lambda x: pd.to_numeric(x, errors="coerce").sum()).fillna(0)
        # reducir dimensiones si es muy grande
        pv = pv.sort_values(by=list(pv.columns), ascending=False).head(30)
        if pv.shape[0]>0 and pv.shape[1]>0:
            from ANALYTICS_ULT.viz import heatmap_pivot
            fig = heatmap_pivot(pv, title="Tama√±o total (bytes)")
            st.pyplot(fig, use_container_width=True)
        st.dataframe(pv, use_container_width=True, height=360)
    else:
        st.info("Se requieren columnas Propietario, Extension y TamanoBytes.")

with tab_time:
    st.subheader("Series temporales")
    for label in ["FechaCreacion","FechaModificacion","FechaAcceso"]:
        if label in df.columns:
            t = timeline_counts(df, label, "M")
            if not t.empty:
                st.markdown(f"**{label} (mensual)**"); st.dataframe(t, use_container_width=True, height=240)
                st.pyplot(smart_time_series(t, "periodo", "conteo", f"Conteo mensual ‚Äî {label}"), use_container_width=True)

with tab_quality:
    st.subheader("Calidad de datos")
    st.dataframe(missingness(df), use_container_width=True, height=360)

with tab_mismatch:
    st.subheader("MIME vs Extensi√≥n")
    st.dataframe(mime_ext_mismatch(df), use_container_width=True, height=420)

with tab_delta:
    st.subheader("Delta (corte vs base)")
    if df_base is None:
        st.info("Cargue un corte base en la barra lateral para activar el delta.")
    else:
        from ANALYTICS_ULT.analyzers import timeline_counts  # reuse
        key = "Hash" if "Hash" in df.columns else ("RutaCompleta" if "RutaCompleta" in df.columns else None)
        if key is None or key not in df_base.columns:
            st.warning("No hay columna clave com√∫n (Hash o RutaCompleta) para delta.")
        else:
            cur = df.drop_duplicates(subset=[key]).set_index(key); base = df_base.drop_duplicates(subset=[key]).set_index(key)
            add_keys = cur.index.difference(base.index); rem_keys = base.index.difference(cur.index); common = cur.index.intersection(base.index)
            add = cur.loc[add_keys].reset_index(); rem = base.loc[rem_keys].reset_index()
            chg = pd.DataFrame({"key": common, "TamanoBytes_cur": pd.to_numeric(cur.loc[common]["TamanoBytes"], errors="coerce"),
                                 "TamanoBytes_base": pd.to_numeric(base.loc[common]["TamanoBytes"], errors="coerce")})
            chg = chg[chg["TamanoBytes_cur"] != chg["TamanoBytes_base"]]
            c1,c2,c3=st.columns(3); c1.metric("Agregados", len(add)); c2.metric("Removidos", len(rem)); c3.metric("Cambiados", len(chg))
            st.markdown("**Agregados**"); st.dataframe(add, use_container_width=True, height=240)
            st.markdown("**Removidos**"); st.dataframe(rem, use_container_width=True, height=240)
            st.markdown("**Cambiados**"); st.dataframe(chg, use_container_width=True, height=240)

with tab_validate:
    st.subheader("Validaciones")
    v1 = validate_sizes(df); v2 = validate_dates(df); v3 = anomalies_size_iqr(df)
    c1,c2,c3=st.columns(3); c1.metric("Tama√±os negativos", len(v1)); c2.metric("Fechas inv√°lidas", len(v2)); c3.metric("Anomal√≠as IQR", len(v3))
    st.markdown("**Fechas inv√°lidas**"); st.dataframe(v2, use_container_width=True, height=240)
    st.markdown("**Tama√±os negativos**"); st.dataframe(v1, use_container_width=True, height=200)
    st.markdown("**Anomal√≠as IQR**"); st.dataframe(v3, use_container_width=True, height=240)

with tab_export:
    st.subheader("Exportes (Excel + PNG + CSV)")
    out_dir = st.text_input("Directorio de salida", value=os.getcwd())
    base_name = st.text_input("Nombre base", value=f"Reporte_Analitica_ULTIMATE_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    if st.button("Generar Excel + Visuales"):
        figures = {}
        if "TamanoBytes" in df.columns:
            f = hist_log_sizes(df["TamanoBytes"]); 
            if f is not None: figures["hist_tamano"] = f
        if "CarpetaPadre" in df.columns:
            tt = df.groupby("CarpetaPadre")["TamanoBytes"].sum().sort_values(ascending=False).head(25)
            tf = treemap_sliced(tt.values, tt.index, title="Treemap - Top carpetas por tama√±o")
            if tf is not None: figures["treemap_carpetas"] = tf
        tables = {
            "ResumenTop": top_n_by_size(df, n=50),
            "CalidadDatos": missingness(df),
            "TopExtensiones": freq_table(df, "Extension", n=50),
            "TopMIME": freq_table(df, "MimeType", n=50),
            "TopPropietario": freq_table(df, "Propietario", n=50),
            "Duplicados": duplicates_by_hash(df)[0],
            "Carpetas": agg_by_folder(df, top=50),
            "TimelineCreacion": timeline_counts(df, "FechaCreacion", "M"),
            "TimelineModificacion": timeline_counts(df, "FechaModificacion", "M"),
            "TimelineAcceso": timeline_counts(df, "FechaAcceso", "M"),
            "MIME_Ext_Mismatch": mime_ext_mismatch(df),
            "RiskTop": risk_scoring(df, json.loads(policies_json)).head(1000),
        }
        from ANALYTICS_ULT.exporters import export_excel_with_figs
        try:
            out_path = export_excel_with_figs(tables, figures, out_dir, base_name)
            st.success(f"Excel generado: {out_path}")
            with open(out_path, "rb") as f:
                st.download_button("Descargar Excel", data=f.read(), file_name=os.path.basename(out_path))
        except Exception as e:
            st.error(f"No se pudo exportar: {e}")
